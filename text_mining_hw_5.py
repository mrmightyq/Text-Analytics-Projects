# -*- coding: utf-8 -*-
"""Text Mining HW 5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zKm5OWM1n8bm0MuhHDOeoZnp7DFfBKo8
"""

# Import all necessary libraries
import pandas as pd
import os
import re
import itertools
import collections
from textblob import TextBlob
from functools import partial
import tweepy as tw
import json 
import pandas as pd
import scipy.stats
import numpy as np
from wordcloud import WordCloud, STOPWORDS 
import nltk
from nltk.corpus import stopwords\

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt

data1 = pd.read_csv('/content/sample_data/MovieReviews.csv')
df = data1

# Textblob tweet sentiment analysis functions
def get_tweet_sentiment_value(Text):
  analysis = TextBlob(Text)
  return analysis.sentiment.polarity

def get_tweet_sentiment_category(Text): 
  ''' 
  Utility function to classify sentiment of passed tweet 
  using textblob's sentiment method 
  '''
  # create TextBlob object of passed tweet text 
  analysis = TextBlob(Text)
  # set sentiment 
  if analysis.sentiment.polarity > 0: 
    return 'positive'
  elif analysis.sentiment.polarity == 0: 
    return 'neutral'
  else: 
    return 'negative'

# Compute the sentiment score and category and add it to the dataframe
df['TextBlob_sentiment_value'] = df['Text'].apply(get_tweet_sentiment_value)
df['TextBlob_sentiment_category'] = df['Text'].apply(get_tweet_sentiment_category)

# Counts of each category for the sentiment, overall
pd.value_counts(df['TextBlob_sentiment_category'])

# Make a boxplot for sentiment distribution by party
bot_ind = (df['Author'] == 'Notabot Jones')
net_ind = (df['Author'] == 'Skylar Net')
plt.boxplot([df['TextBlob_sentiment_value'][bot_ind], df['TextBlob_sentiment_value'][net_ind]])
plt.xticks(ticks=[1,2], labels=['Notabot Jones', 'Skylar Net'])
plt.title("Sentiment Distribution by Author")
plt.show()





# Aggregate counts of sentiment category by party
sent_counts = df['TextBlob_sentiment_category'].value_counts()
positive_counts = [sent_counts['positive']]
#neutral_counts = [sent_counts['neutral']]
negative_counts = [sent_counts['negative']]

# Create a barplot of the sentiment category counts by party
barWidth = 0.25
r1 = np.arange(1)
#r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r1]

plt.bar(r1, negative_counts, color='#ff8080', width=barWidth, edgecolor='white', label='Negative')
#plt.bar(r2, neutral_counts, color='#808080', width=barWidth, edgecolor='white', label='Neutral')
plt.bar(r3, positive_counts, color='#8080ff', width=barWidth, edgecolor='white', label='Positive')

plt.xlabel('Sentiment', fontweight='bold')
plt.xticks([r + barWidth for r in range(0)], ['Sentiment'])

plt.title('Expected Sentiment by Author')
plt.legend()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Python program to generate WordCloud 
# %matplotlib inline
from matplotlib import pyplot as plt
#
# 

comment_words = '' 
custom_stopwords = ["go", "going"]
stopwords = set(STOPWORDS).union(custom_stopwords) 

# iterate through the csv file 
for val in df.Text: 
	# typecaste each val to string 
	val = str(val) 

	# split the value 
	tokens = val.split() 
	
	# Converts each token into lowercase 
	for i in range(len(tokens)): 
		tokens[i] = tokens[i].lower() 
	
	comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white', 
				stopwords = stopwords, 
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

!pip install afinn
from afinn import Afinn
afinn = Afinn(language='en')
afinn_wl_url = ('https://raw.githubusercontent.com'
                '/fnielsen/afinn/master/afinn/data/AFINN-111.txt')
afinn_wl_df = pd.read_csv(afinn_wl_url,
                          header=None, # no column names
                          sep='\t',  # tab sepeated
                          names=['term', 'value']) #new column names

seed = 808 # seed for sample so results are stable
afinn_wl_df.sample(10, random_state = seed)

df['afinn_score'] = df['Text'].apply(afinn.score)

pd.set_option('max_colwidth', 100)

columns_to_display = ['Text', 'afinn_score']

df.sort_values(by='afinn_score')[columns_to_display].head(10)

df.sort_values(by='afinn_score')[columns_to_display].head(10)

!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def vaderize(df, textfield):
    '''Compute the Vader polarity scores for a textfield.
    Returns scores and original dataframe.'''

    analyzer = SentimentIntensityAnalyzer()

    print('Estimating polarity scores for %d cases.' % len(df))
    sentiment = df[textfield].apply(analyzer.polarity_scores)

    # convert to dataframe
    sdf = pd.DataFrame(sentiment.tolist()).add_prefix('vader_')

    # merge dataframes
    df_combined = pd.concat([df, sdf], axis=1)
    return df_combined

df_vaderized = vaderize(df, 'Text')
df_vaderized.head()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

df_vaderized['vader_compound'].plot(kind='hist')

df_vaderized.plot.scatter(x='vader_pos', y = 'vader_neg')

df_vaderized.to_csv('/content/AI Sentiment Classification.csv')

data2 = pd.read_csv('/content/AI Sentiment Classification.csv')
df = data2

# Aggregate counts of sentiment category by party
afinn_sent_counts = df['afinn_score'].value_counts()
text_sent_counts = df['TextBlob_sentiment_category'].value_counts()
vader_sent_counts = df['vader_compound'].value_counts()
positive_counts = [afinn_sent_counts['positive'], text_sent_counts['positive'],vader_sent_counts['positive']]
neutral_counts = [afinn_sent_counts['neutral'], 0, vader_sent_counts['neutral']]
negative_counts = [afinn_sent_counts['negative'], text_sent_counts['negative'],vader_sent_counts['negative']]



# Create a barplot of the sentiment category counts by party
barWidth = 0.25
r1 = np.arange(3)
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]

plt.bar(r1, negative_counts, color='#ff8080', width=barWidth, edgecolor='white', label='Negative')
plt.bar(r2, neutral_counts, color='#808080', width=barWidth, edgecolor='white', label='Neutral')
plt.bar(r3, positive_counts, color='#8080ff', width=barWidth, edgecolor='white', label='Positive')

plt.xlabel('Library', fontweight='bold')
plt.xticks([r + barWidth for r in range(3)], ['AFINN', 'TextBlob','Vader'])

plt.title('Statement Sentiment by Library')
plt.legend()
plt.show()

!pip install krippendorff
import krippendorff

arr = [['p','p','p','n','n','n'],
       ['p','p','neu','n','p','neu'],
       ['p','p','neu','n','n','neu']]
    
arr2 = [[1,1,1,3,3,3],
       [1,1,2,3,1,2],
       [1,1,2,3,3,2]] 

res = krippendorff.alpha(arr2)
print(res)

arr = [[1, 3, 1, 3, 3, 1],
       [1, 3, 1, 3, 3, 1],
       [1, 3, 1, 3, 3, 1],
       [1, 3, 1, 3, 2, 1],
       [1, 3, 1, 3, 2, 1],
       [1, 3, 1, 3, 2, 1]]    
res = krippendorff.alpha(arr)
print(res)

from sklearn.metrics import cohen_kappa_score
y1 = ['p','p','p','n','n','n']
y2 = ['p','p','neu','n','p','neu']
y3 = ['p','p','neu','n','n','neu']
k1=cohen_kappa_score(y1,y2)
k2=cohen_kappa_score(y1,y3)
k3=cohen_kappa_score(y3,y2)
print((k1+k2+k3)/3)
print(k1)
print(k2)
print(k3)

y1 = [1, 3, 1, 3, 3, 1]
y2 = [1, 3, 1, 3, 3, 1]
y3 = [1, 3, 1, 3, 3, 1]
y4 = [1, 3, 1, 3, 2, 1]
y5 = [1, 3, 1, 3, 2, 1]
y6 = [1, 3, 1, 3, 2, 1]
k1=cohen_kappa_score(y1,y2)
k2=cohen_kappa_score(y1,y3)
k3=cohen_kappa_score(y1,y4)
k4=cohen_kappa_score(y1,y5)
k5=cohen_kappa_score(y1,y6)
k6=cohen_kappa_score(y2,y3)
k7=cohen_kappa_score(y2,y4)
k8=cohen_kappa_score(y2,y5)
k9=cohen_kappa_score(y2,y6)
k10=cohen_kappa_score(y3,y4)
k11=cohen_kappa_score(y3,y5)
k12=cohen_kappa_score(y3,y6)
k13=cohen_kappa_score(y4,y5)
k14=cohen_kappa_score(y4,y6)
k15=cohen_kappa_score(y5,y6)


avg_kap = (k1+k2+k3+k4+k5+k6+k7+k8+k9+k10+k11+k12+k13+k14+k15)/15
print(avg_kap)

y =   [[0, 0, 1, 0, 0, 1],
       [1, 0, 1, 0, 0, 1],
       [1, 0, 1, 0, 0, 1],
       [1, 0, 1, 0, 3, 1],
       [1, 0, 1, 0, 3, 1],
       [1, 0, 1, 0, 3, 1]]

k = []
for i in range(len(y)):
  for j in range(i,len(y)):
    if i == j:
       continue
    k.append(cohen_kappa_score(y[i],y[j]))
n = 10

avg_kappa = sum(k)/n
 print(avg_kappa)

y =   [[3, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 3, 3],
       [2, 1, 1, 3, 1, 3],
       [3, 1, 1, 1, 3, 2],
       [3, 1, 1, 1, 3, 3]]  
res = krippendorff.alpha(y)
print(res)